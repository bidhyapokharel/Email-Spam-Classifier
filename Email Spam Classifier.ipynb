{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d56fa2d",
   "metadata": {},
   "source": [
    "\n",
    "# Spam email classifier\n",
    "In this project, I build a spam email classifier that can tell whether a given email is a spam email or not based on the emailâ€™s content.\n",
    "\n",
    "There four main parts:\n",
    "- Clean the data\n",
    "- build feature vectors\n",
    "- Use Naive Bayes Classifier, SVM(linear kernel) and Logistic Regression to training the data and make predictions\n",
    "- Compare these models and discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb413622",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6e420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1373b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spam and ham emails data\n",
    "# Source is https://www.kaggle.com/veleon/ham-and-spam-dataset\n",
    "\n",
    "PATH = 'hamnspam/'\n",
    "ham_filenames = [ name for name in os.listdir(PATH+'ham')]\n",
    "spam_filenames = [ name for name in os.listdir(PATH+'spam')]\n",
    "\n",
    "random.shuffle(ham_filenames)\n",
    "random.shuffle(spam_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c0d9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails: 458\n",
      "Number of ham emails: 2548\n"
     ]
    }
   ],
   "source": [
    "#load emails\n",
    "def load_email(is_spam,filename,path):\n",
    "    '''\n",
    "    Load email by its name and loacation\n",
    "    \n",
    "    @is_spam: whether email is spam or not\n",
    "    @filename: email name\n",
    "    @path: PATH \n",
    "    \n",
    "    @return: email object\n",
    "    '''\n",
    "    if is_spam :\n",
    "        path = path + 'spam/' + filename\n",
    "    else :\n",
    "        path = path + 'ham/' + filename\n",
    "    with open(path,'rb') as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "ham_emails = [load_email(False,name,PATH) for name in ham_filenames]\n",
    "spam_emails = [load_email(True,name,PATH) for name in spam_filenames]\n",
    "\n",
    "#remove not text type emails\n",
    "ham_emails = [email for email in ham_emails if type(email.get_payload()) is str or len(email.get_payload())>1] \n",
    "spam_emails = [email for email in spam_emails if type(email.get_payload()) is str or len(email.get_payload())>1]\n",
    "\n",
    "print('Number of spam emails:',len(spam_emails))\n",
    "print('Number of ham emails:',len(ham_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3ee66c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example email content:\n",
      "\n",
      " 5, Meridian East\n",
      "Leicester LE3 2WZ \n",
      "Leicester City, \n",
      "United Kingdom\n",
      "Tel/Fax: 44-870-136-7079\n",
      "\n",
      "Date: 09/11/2002 \n",
      "\n",
      "FROM: CAPT JOHN OKELE \n",
      "\n",
      "DEAR SIR, \n",
      "\n",
      "I got your contact through a Military friend who I did training with in your country, I am Captain JOHN OKELE the former Commander of the Security Guards of the late Laurent Kabila (former President, Democratic Republic of Congo (Africa), I am present\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "test_email = spam_emails[0]\n",
    "print('Example email content:\\n\\n',test_email.get_payload()[:400])  #limit the size to 400 chars\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914af1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bidhya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stem words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f256402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process email's content\n",
    "import re\n",
    "import string\n",
    "\n",
    "def process_email(content) :\n",
    "    '''\n",
    "    preprocesses the content of an email \n",
    "    \n",
    "    and returns a dictionary with word as key and its frequency as value\n",
    "    @content : email content (a string)\n",
    "    @return : a counting dictionary \n",
    "    '''                                         \n",
    "    if not isinstance(content,str) :       \n",
    "        return {},''\n",
    "    content = re.sub(r'<[^<>]+>', ' ',content)  ##strip all HTML\n",
    "    content = str.lower(content) ##lower case\n",
    "    \n",
    "    #handle URLS with http:// or https://\n",
    "    content = re.sub(r'(http|https)://[^\\s]*','httpaddr ',content) \n",
    "    \n",
    "    #handle email addresses\n",
    "    #look for strings with @ in the middle\n",
    "    content = re.sub(r'[^\\s]+@[^\\s]+','emailaddr',content)\n",
    "    \n",
    "    content = re.sub(r'[0-9]+', 'number ',content) #handle numbers\n",
    "    content = re.sub(r'[$]+','dollar ',content) #handle $ sign \n",
    "    content = re.sub(r'[\\n]+',' ',content) #remove \\n\n",
    "    #remove punctuaion\n",
    "    content = re.sub(r'[{0}]'.format(string.punctuation),' ',content) \n",
    "    \n",
    "    res = {}\n",
    "    words = word_tokenize(content)\n",
    "    content = ' '.join([ps.stem(word) for word in words])\n",
    "    for word in words :\n",
    "        word = ps.stem(word)\n",
    "        if len(word) > 11 :\n",
    "            continue\n",
    "        if len(word) <=1 :\n",
    "            continue\n",
    "        if not res.get(word):\n",
    "            res[word] = 0\n",
    "        res[word] += 1  \n",
    "    return res,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d87e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email content becomes(only list 500 chars):\n",
      "\n",
      " number meridian east leicest lenumb number wz leicest citi unit kingdom tel fax number number number number date number number number from capt john okel dear sir i got your contact through a militari friend who i did train with in your countri i am captain john okel the former command of the secur guard of the late laurent kabila former presid democrat republ of congo africa i am present in self exil with my wife and one child in remot leicest citi britain due to threat of the present govern he\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(res,content) = process_email(test_email.get_payload())\n",
    "print('Email content becomes(only list 500 chars):\\n\\n',process_email(test_email.get_payload())[1][:500]) \n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fd1603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total words in spam emails: 44304\n",
      "number of words that with frequency than 11: 2049\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary for spam emails\n",
    "def build_vocab(emails) :\n",
    "    '''\n",
    "    build_vocab will build a vocabulary with words \n",
    "    appearing in the email content\n",
    "    @emails : list of email\n",
    "    @return : \n",
    "    '''\n",
    "    assert isinstance(emails,list)\n",
    "    dic = {}\n",
    "    index = 0\n",
    "    \n",
    "    while index < len(emails) :\n",
    "        email = emails[index]\n",
    "        judge = email.get_payload()\n",
    "        if type(judge) is not str:\n",
    "            dict_email = {}\n",
    "            for e in judge :\n",
    "                dic_toadd = process_email(e.get_payload())\n",
    "                for word in dic_toadd[0] :\n",
    "                    if not dic_email.get(word):\n",
    "                        dic_email[word] = 0\n",
    "                    dic_email[word] += 1\n",
    "        else :\n",
    "            dic_email = process_email(judge)[0]\n",
    "        for word in dic_email.keys() :\n",
    "            if not dic.get(word) :\n",
    "                dic[word] = 0\n",
    "            dic[word] += dic_email[word]\n",
    "        index+=1\n",
    "    \n",
    "    return dic\n",
    "# print(test_email['Subject'])\n",
    "vocab = build_vocab(spam_emails)\n",
    "print('number of total words in spam emails:', len(vocab.keys()))\n",
    "vocab = [word for word in vocab.keys() if vocab[word]>11]\n",
    "print('number of words that with frequency than 11:', len(vocab))\n",
    "n = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed55e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export vocabulary to a csv file\n",
    "df_vo = pd.DataFrame(vocab)\n",
    "df_vo = df_vo.rename(columns={0:'words'})\n",
    "df_vo.to_csv('vocabulary.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d7a61",
   "metadata": {},
   "source": [
    "\n",
    "# Part 2 - build feature vectors\n",
    "\n",
    "Feature vectors are constructed by the vocabulary with each column standing for a word in vocabulary\n",
    "\n",
    "Based on eamil content, if email contains a certain word, then in its feature vector, the value of that word's location will be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b1473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class emailToFeature:\n",
    "    '''\n",
    "    This is a class for building feature vectors\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,filename) :\n",
    "        vocab = pd.read_csv(filename)\n",
    "        vocab = list(vocab['words'])\n",
    "        index = 0\n",
    "        vocabulary = {}\n",
    "        while index < len(vocab) :\n",
    "            vocabulary[vocab[index]] = index\n",
    "            index+=1\n",
    "        self.d = len(vocab)\n",
    "        self.vocab = vocabulary   \n",
    "    \n",
    "    def fea_vector(self,email) :\n",
    "        '''\n",
    "        return a numpy array(1Xn) representing the\n",
    "        feature vector\n",
    "        @email: input email can be both a string and email object\n",
    "        '''\n",
    "        if type(email) is str:\n",
    "            judge = email\n",
    "        else :\n",
    "            judge = email.get_payload()\n",
    "        if not type(judge) is str:\n",
    "            dic_email = {}\n",
    "            for e in judge :\n",
    "                dic_toadd = process_email(e.get_payload())\n",
    "                for word in dic_toadd[0] :\n",
    "                    if not dic_email.get(word):\n",
    "                        dic_email[word] = 0\n",
    "                    dic_email[word] += 1\n",
    "        else :\n",
    "            dic_email = process_email(judge)[0]\n",
    "            res = np.zeros((1,self.d))\n",
    "        for word in dic_email.keys() :\n",
    "            if not self.vocab.get(word):\n",
    "                continue\n",
    "            index = self.vocab[word]\n",
    "            res[0,index] = 1\n",
    "        return res\n",
    "    \n",
    "    def build_vectors(self,is_spam,emails) :\n",
    "        '''\n",
    "        build feature vectors\n",
    "        \n",
    "        @emails : list of ham or spam emails\n",
    "        @return : numpy array representing feature vectors\n",
    "        '''\n",
    "        N = len(emails)  # N*d array\n",
    "        fea_vectors = np.zeros((N,self.d+1))\n",
    "        for i in range(N) :\n",
    "            a = self.fea_vector(emails[i])\n",
    "            fea_vectors[i,:-1] = a\n",
    "        if is_spam :\n",
    "            fea_vectors[:,self.d] = 1\n",
    "        else :\n",
    "            fea_vectors[:,self.d] = 0\n",
    "        return fea_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ed2069",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'res' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1721ecdcbf5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memailTof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memailToFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocabulary.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#class object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspam_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memailTof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspam_emails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mham_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memailTof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mham_emails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-04cfc5a67a7f>\u001b[0m in \u001b[0;36mbuild_vectors\u001b[0;34m(self, is_spam, emails)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mfea_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfea_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mfea_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_spam\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-04cfc5a67a7f>\u001b[0m in \u001b[0;36mfea_vector\u001b[0;34m(self, email)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#             res[0,index] = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_spam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memails\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'res' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#construct feature vectors and export them to csv files\n",
    "\n",
    "emailTof = emailToFeature('vocabulary.csv') #class object\n",
    "spam_vectors = emailTof.build_vectors(True,spam_emails)\n",
    "ham_vectors = emailTof.build_vectors(False,ham_emails)\n",
    "\n",
    "index = list(range(n+1))\n",
    "spam_df = pd.DataFrame(spam_vectors,columns=index)\n",
    "spam_df.to_csv('spam_vectors.csv',index = False)\n",
    "ham_df = pd.DataFrame(ham_vectors,columns=index)\n",
    "ham_df.to_csv('ham_vectors.csv',index=False)\n",
    "\n",
    "print('size of spam feature vectors is:',spam_vectors.shape)\n",
    "print('size of ham feature vectors is:',ham_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087586a9",
   "metadata": {},
   "source": [
    "# Split the data to training set, cross-validation set and test set (60%,20%,20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26246c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to training set, cross-validation set and test set (60%,20%,20%)\n",
    "ham_training = ham_vectors[:1530]\n",
    "ham_validation,ham_test = ham_vectors[1530:2039],ham_vectors[2039:]\n",
    "\n",
    "spam_training = spam_vectors[:274]\n",
    "spam_validation,spam_test = spam_vectors[274:365],spam_vectors[365:]\n",
    "\n",
    "training = np.concatenate((ham_training,spam_training))\n",
    "np.random.shuffle(training)\n",
    "pd.DataFrame(training,columns=index).to_csv('training.csv',index=False)\n",
    "\n",
    "cval = np.concatenate((ham_validation,spam_validation))\n",
    "test = np.concatenate((ham_test,spam_test))\n",
    "pd.DataFrame(cval,columns=index).to_csv('cross-validation.csv',index=False)\n",
    "pd.DataFrame(test,columns=index).to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38376c3a",
   "metadata": {},
   "source": [
    "# Part 3 - use models to train the data\n",
    "First model: naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447cb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get word frequency distribution in both spam and non-spam email\n",
    "#60% training data and 20% test data, the same as above\n",
    "#Only use the word whose frequency difference between spam and non-spam email is greater than 20\n",
    "vocab_spam = build_vocab(spam_emails[0:274])\n",
    "vocab_ham = build_vocab(ham_emails[0:1530])\n",
    "voc = [word for word in vocab_spam.keys() if vocab_ham.get(word) and abs(vocab_ham[word] - vocab_spam[word]) > 20] \n",
    "vocab_ham = {word:vocab_ham[word] for word in voc}\n",
    "vocab_spam = {word:vocab_spam[word] for word in voc}\n",
    "sum_ham,sum_spam = sum(list(vocab_ham.values())),sum(list(vocab_spam.values()))\n",
    "vocab_ham = {word:vocab_ham[word]/sum_ham for word in voc}   # key is word and value is probability in ham eamils\n",
    "vocab_spam = {word:vocab_spam[word]/sum_spam for word in voc} # key is word and value is probability in spam eamils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20%test set\n",
    "spam_test,ham_test = spam_emails[365:],ham_emails[2039:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95230e39",
   "metadata": {},
   "source": [
    "# Visualize word frequency distribution of spam and non-spam email respectivelyÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674068ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_spam = list(vocab_spam.keys())\n",
    "word_spam.sort(key=lambda x:vocab_spam[x],reverse=True)\n",
    "word_spam = word_spam[:100] #choose 100 words\n",
    "y_spam, y_ham= [vocab_spam[word] for word in word_spam],[vocab_ham[word] for word in word_spam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f65471",
   "metadata": {},
   "outputs": [],
   "source": [
    "In [15]:\n",
    "plt.plot(list(range(100)),y_spam)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Index of word(only show 100 words)')\n",
    "plt.title('Word frequency probability distribution(spam email)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(100)),y_ham)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Index of word(only show 100 words)')\n",
    "plt.title('Word frequency probability distribution(non-spam)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single(email):\n",
    "    '''\n",
    "    Return a str which represents processed email content\n",
    "    '''\n",
    "    judge = email.get_payload()\n",
    "    dic = {}\n",
    "    if type(judge) is not str:\n",
    "        dic_email = {}\n",
    "        for e in judge :\n",
    "            dic_toadd = process_email(e.get_payload())\n",
    "            for word in dic_toadd[0] :\n",
    "                if not dic_email.get(word):\n",
    "                    dic_email[word] = 0\n",
    "                dic_email[word] += 1\n",
    "    else :\n",
    "        dic_email = process_email(judge)[0]\n",
    "    for word in dic_email.keys() :\n",
    "        if not dic.get(word) :\n",
    "            dic[word] = 0\n",
    "        dic[word] += dic_email[word]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b37f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to compute precision, recall, f1-scrore and error\n",
    "def com_precision(y,y_p):\n",
    "    '''\n",
    "    y is true classification\n",
    "    y_p is predicted classification\n",
    "    '''\n",
    "    length = len(y)\n",
    "    num_pre1 = np.sum(y_p)\n",
    "    num_correct = 0\n",
    "    for i in range(length) :\n",
    "        if y_p[i] != 1:\n",
    "            continue\n",
    "        if y[i] == 1:\n",
    "            num_correct += 1\n",
    "    return num_correct / num_pre1\n",
    "\n",
    "def com_recall(y,y_p) :\n",
    "    '''\n",
    "    y is true classification\n",
    "    y_p is predicted classification\n",
    "    '''\n",
    "    actual1 = np.sum(y)\n",
    "    length = len(y)\n",
    "    num_correct = 0\n",
    "    for i in range(length) :\n",
    "        if y[i]!=1 :\n",
    "            continue \n",
    "        if y_p[i] == 1:\n",
    "            num_correct += 1\n",
    "    return num_correct / actual1\n",
    "\n",
    "def com_f1Score(precision,recall):\n",
    "    '''\n",
    "    Compute F1-score\n",
    "    ''' \n",
    "    return 2*precision*recall/(precision+recall)\n",
    "\n",
    "def com_error(y,y_p):\n",
    "    '''\n",
    "    compute error\n",
    "    '''\n",
    "    vali = np.ones(len(y))\n",
    "    vali = vali[y == y_p]\n",
    "    return 1-np.sum(vali) / len(y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e1f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Predict\n",
    "indexs_spam = np.array([0]*(len(spam_test)+len(ham_test))) #this will be the predicted array \n",
    "judge_array = np.array([1]*len(spam_test)+[0]*len(ham_test)) # this is the true value array\n",
    "emails_t = spam_test + ham_test\n",
    "for i in range(len(spam_test)+len(ham_test)):\n",
    "    email = emails_t[i]\n",
    "    con = process_single(email)\n",
    "    con = {word:con[word] for word in con if vocab_spam.get(word)}\n",
    "    df = pd.DataFrame.from_dict(con,orient='index',columns=['frequency'])\n",
    "    prob_spam = np.array([vocab_spam[word] for word in df.index if vocab_spam.get(word)])\n",
    "    prob_spam = np.log(prob_spam)\n",
    "    df['prob_spam'] = prob_spam\n",
    "    prob_ham = np.array([vocab_ham[word] for word in df.index if vocab_spam.get(word)])\n",
    "    prob_ham = np.log(prob_ham)\n",
    "    df['prob_ham'] = prob_ham\n",
    "    if ((df['prob_spam'] * df['frequency']).sum()+np.log(0.1524) >= (df['prob_ham'] * df['frequency']).sum()+np.log(1-0.1524)) :\n",
    "        indexs_spam[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d374027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
